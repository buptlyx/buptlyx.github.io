<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>从0开始的大语言模型之路(1)</title>
    <url>/2025/12/01/llm1/</url>
    <content><![CDATA[<h3 id="故事背景"><a href="#故事背景" class="headerlink" title="故事背景"></a>故事背景</h3><p>&emsp;&emsp;这学期，我们的神经网络课程设计大作业是经典的图像生成文字（ImageCaption）任务。直到今天，我才刚刚着手开始写代码，画了一些时间把数据集整理好，然后把任务需求统统塞给<strong>Gemini3Pro</strong>后，Gemini只花了不到半分钟的时间就给我生成了所有的代码，并且能完整运行不报错。虽然我深知今日的大语言模型在编程方面已经相当成熟，但是还是不由得感叹我们堂堂北邮人工智能专业，最核心专业课的课程大作业居然被AI给秒了？虽然受限于笔记本的显卡性能，Gemini给我的代码让我从下午五点跑到了晚上八点，但是这激发了整个学期只是混个考勤的我，对于语言模型的兴趣，于是便有了这一篇文章，甚至这篇博客都是为了写这篇文章专门搭建的。</p>
<p>&emsp;&emsp;我打算从最最基础的开始学起，同时遵循我的偶像<strong>理查德·费曼</strong>的学习方法，边学边把学到的东西用最通俗易懂的方式讲出来。</p>
<h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h3><p>&emsp;&emsp;在深入探讨语言模型的原理前，我们不妨先抛出一个问题：</p>
<blockquote>
<p>语言模型是如何处理我们输入的问题，并输出我们想要的回答的？</p>
</blockquote>
<p>&emsp;&emsp;首先，对于我们输入的一段文字，例如是*把大象装进冰箱里需要几步？*这一段话，语言模型先会把这一段话利用预先训练好的分词器，进行分词处理（Tokenizations），被切成[‘把’,’大象’,’’装’,进’,’冰箱’,’里’,’需要’,’几’,’步’,’？’] 。这一个个用逗号分开的字符串又被称为”<strong>Token</strong>“。而语言模型本身是无法直接读取自然语言的信息的，所以在这一步后，模型会按照设定好的词表（Vocabulary），把它们转换为整型数，例如[189,24,43,…,91]，这些整数没有任何含义，只是用于映射到某个词的索引（index）。</p>
<p>&emsp;&emsp;接着，这些Token，或者说转换后的整型数，会被赋予一个独一无二的“身份证”——<strong>词向量</strong>。在词向量中，蕴含着这个Token一切的语义信息，这个的过程，就是<strong>词嵌入</strong>（Embedding）。词向量也是向量，那么不同的词向量理所应当就可以进行数值上的计算。一个很流行的说法是：“国王”-“男人”+“女人”&#x3D;“皇后”。虽然这被许多人验证指出这并不是一个严谨的说法，但是我们可以管中窥豹，以此来理解词向量的用途。在3Blue1Brown的视频中对此给出了一个解释，皇后（Queen）的含义并不仅限于“女性的国王”这一层语义，可能还表示皇后乐队、善于社交的人等不同的语义。</p>
<p>&emsp;&emsp;当然，在一些词语的常见语义和在句子中的语义是有些差别的。还是举皇后的例子，单独一个词“皇后”，那么它的语义可能80%是“女性的国王”这一层。但是如果把“皇后”放到“乐队”前面，那么这个“皇后”的语义就发生改变了。在自然语言沟通中，这个道理是基础的、原则性的、会被人类大脑普遍接受的。那么，为了让模型更好地模拟这一过程，我们引入了一个强大的工具——<strong>注意力机制</strong>（Attention）。在自然语言处理领域最有建设性、最有代表性、最出圈的一篇论文《Attention is all you need》中，对注意力机制有更完整的阐述。后续我也出更多的文章来单独讲解注意力机制。现在我们不需要理解注意力机制有什么深奥的数学原理，只需要懂运用注意力机制可以让词语在上下文中有更准确的含义就行。同时，我们还需要一个<strong>位置编码</strong>（Positional encoding）来让模型知道句子中词语出现的顺序。</p>
<p>&emsp;&emsp;然后，一个很自然的想法就出现了：<em>我明白了模型如何处理我的输入，那么语言模型是如何输出的呢？</em> 这个问题其实才是理解语言模型原理的关键，而这个问题的答案也很简单，就是两个字“<strong>接龙</strong>”，语言模型做的工作，不过就是接收到你传入的句子，然后经过一系列计算，得到你的句子下一个字的出现的概率，然后根据概率并采取一定策略进行输出。然后将生成的字与原先的输入拼接在一起再重新输入到模型中，计算概率得到新的字，循环往复，直到出现特殊token结束符为止。</p>
<p>&emsp;&emsp;至此，一个大多数语言模型的对于输入句子的处理就完成了。那么回到最开始的问题“<strong>把大象装进冰箱里需要几步？</strong>”，Gemini3Pro的回答是：<br>要把大象装进冰箱，总共分三步：</p>
<ol>
<li>把冰箱门打开。</li>
<li>把大象装进去。</li>
<li>把冰箱门带上。</li>
</ol>
<h3 id="简单聊聊"><a href="#简单聊聊" class="headerlink" title="简单聊聊"></a>简单聊聊</h3><p>以上是我目前对于语言模型一些能立马说出来的认识，当然这篇文章只是一个开篇综述，没有任何的代码或者数学推导，不过没有关系，我想要写的就是大部分人能看得懂的、入门级的<strong>偏教学类</strong>文章。后续也希望我可以继续更新，深入学习。</p>
]]></content>
      <tags>
        <tag>大模型,深度学习,Attention</tag>
      </tags>
  </entry>
</search>
